{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1:  What is K-Nearest Neighbors (KNN) and how does it work in both\n",
        "classification and regression problems?**\n",
        "\n",
        "\n",
        "Ans ="
      ],
      "metadata": {
        "id": "AYL56d7XPfRR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6378a28f"
      },
      "source": [
        "**Question 1:  What is K-Nearest Neighbors (KNN) and how does it work in both\n",
        "classification and regression problems?**\n",
        "\n",
        "Ans = K-Nearest Neighbors (KNN) is a non-parametric, lazy learning algorithm that can be used for both classification and regression tasks.\n",
        "\n",
        "**How it works:**\n",
        "\n",
        "1.  **Training:** In the training phase, KNN simply stores the entire training dataset. There is no explicit model building.\n",
        "\n",
        "2.  **Prediction:** To make a prediction for a new data point:\n",
        "    *   Calculate the distance (usually Euclidean distance) between the new data point and all data points in the training set.\n",
        "    *   Select the 'k' nearest neighbors (data points with the smallest distances).\n",
        "    *   **For Classification:** The predicted class for the new data point is the class that is most frequent among the 'k' nearest neighbors.\n",
        "    *   **For Regression:** The predicted value for the new data point is the average (or median) of the values of the 'k' nearest neighbors.\n",
        "\n",
        "**Key aspects:**\n",
        "\n",
        "*   **Parameter 'k':** The choice of 'k' is crucial. A small 'k' can be sensitive to noise, while a large 'k' can smooth out the decision boundary but may include data points from other classes/values.\n",
        "*   **Distance Metric:** The distance metric used can influence the results (e.g., Euclidean, Manhattan).\n",
        "*   **Computational Cost:** Prediction can be computationally expensive, especially for large datasets, as it requires calculating distances to all training points.\n",
        "*   **No Explicit Model:** KNN is a \"lazy\" learner because it doesn't build an explicit model during training. The model is essentially the entire training dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: What is the Curse of Dimensionality and how does it affect KNN\n",
        "performance?**\n",
        "\n",
        "Ans ="
      ],
      "metadata": {
        "id": "XN9Dq9YmQhM6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e5df050"
      },
      "source": [
        "**Question 2: What is the Curse of Dimensionality and how does it affect KNN\n",
        "performance?**\n",
        "\n",
        "Ans = The **Curse of Dimensionality** refers to the various difficulties that arise when working with data in high-dimensional spaces (i.e., data with a large number of features or attributes). As the number of dimensions increases, the volume of the space grows exponentially, and the available data becomes sparse within this vast space.\n",
        "\n",
        "**How it affects KNN performance:**\n",
        "\n",
        "*   **Increased Sparsity:** In high dimensions, data points become increasingly sparse. This means that the distance between any two points tends to be large, and the concept of \"nearest neighbors\" becomes less meaningful. All points can appear to be far away from each other.\n",
        "*   **Distance Metrics Become Less Reliable:** In high dimensions, the difference in distance between the nearest and farthest neighbors can become smaller. This makes it harder to distinguish between neighbors and non-neighbors based on distance, leading to less reliable neighborhood identification.\n",
        "*   **Increased Computational Cost:** Calculating distances between points in high dimensions is computationally more expensive.\n",
        "*   **Overfitting:** With sparse data in high dimensions, KNN is more prone to overfitting. The model might find spurious patterns in the training data that don't generalize well to new, unseen data.\n",
        "*   **Need for More Data:** To maintain the same density of data points in a high-dimensional space as in a lower-dimensional space, an exponentially larger amount of data is required. This is often impractical or impossible to obtain.\n",
        "\n",
        "In summary, the Curse of Dimensionality makes it difficult for KNN to find true nearest neighbors, reduces the reliability of distance metrics, increases computational cost, and makes the model more susceptible to overfitting in high-dimensional spaces. This is why dimensionality reduction techniques are often used before applying KNN to high-dimensional data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3: What is Principal Component Analysis (PCA)? How is it different from\n",
        "feature selection?**\n",
        "\n",
        "ans ="
      ],
      "metadata": {
        "id": "XHD8PojSRVCC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cf930d2"
      },
      "source": [
        "**Question 3: What is Principal Component Analysis (PCA)? How is it different from\n",
        "feature selection?**\n",
        "\n",
        "ans = **Principal Component Analysis (PCA)** is a dimensionality reduction technique that transforms a set of possibly correlated variables into a set of linearly uncorrelated variables called principal components. It does this by finding the directions (principal components) in the data that capture the most variance. The first principal component captures the most variance, the second captures the second most, and so on. By keeping only a subset of the principal components, we can reduce the dimensionality of the data while retaining as much of the original variance as possible.\n",
        "\n",
        "**How it's different from feature selection:**\n",
        "\n",
        "*   **PCA (Feature Extraction):** PCA creates new, synthetic features (principal components) that are linear combinations of the original features. The original features are transformed into a new set of features.\n",
        "*   **Feature Selection:** Feature selection methods choose a subset of the original features to keep, discarding the rest. The features that are kept are the original features themselves.\n",
        "\n",
        "In essence, PCA *extracts* new features, while feature selection *selects* existing features. PCA is useful when you want to reduce dimensionality while preserving as much information (variance) as possible, and the new, transformed features are acceptable. Feature selection is useful when you want to work with the original features, perhaps for interpretability or because the relationships between the original features are important."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4: What are eigenvalues and eigenvectors in PCA, and why are they\n",
        "important?**\n",
        "\n"
      ],
      "metadata": {
        "id": "bf-qT0IbRl6P"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a978fc1"
      },
      "source": [
        "**Question 4: What are eigenvalues and eigenvectors in PCA, and why are they\n",
        "important?**\n",
        "\n",
        "Ans = In the context of PCA:\n",
        "\n",
        "*   **Eigenvectors:** These are the directions or principal components along which the data varies the most. They are the directions of maximum variance in the data. In PCA, the eigenvectors of the covariance matrix of the data represent the principal components.\n",
        "*   **Eigenvalues:** These are the magnitudes or \"strengths\" associated with each eigenvector. They indicate the amount of variance in the data that is captured by its corresponding eigenvector. A larger eigenvalue means that the corresponding eigenvector captures more of the data's variance.\n",
        "\n",
        "**Why they are important in PCA:**\n",
        "\n",
        "*   **Determining Principal Components:** Eigenvectors define the principal components. The eigenvector with the largest eigenvalue is the first principal component, the eigenvector with the second largest eigenvalue is the second principal component, and so on.\n",
        "*   **Quantifying Variance:** Eigenvalues allow us to quantify how much variance is explained by each principal component. This is crucial for dimensionality reduction, as we can choose to keep only the principal components (eigenvectors) that correspond to the largest eigenvalues, thereby retaining the most important information (variance) in the data while reducing the number of dimensions.\n",
        "*   **Ordering Principal Components:** Eigenvalues provide a way to order the principal components in terms of their importance (how much variance they capture). This ordering helps in deciding how many principal components to keep to achieve a desired level of dimensionality reduction and information retention.\n",
        "\n",
        "In essence, eigenvalues and eigenvectors provide the mathematical foundation for PCA. They allow us to identify the most significant directions of variation in the data and quantify their importance, which is essential for dimensionality reduction while preserving as much information as possible."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "****"
      ],
      "metadata": {
        "id": "zOWuAWmtQyQU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5: How do KNN and PCA complement each other when applied in a single\n",
        "pipeline?**\n",
        "\n",
        "Ans ="
      ],
      "metadata": {
        "id": "PGKyBrN5RyKE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33343981"
      },
      "source": [
        "**Question 5: How do KNN and PCA complement each other when applied in a single\n",
        "pipeline?**\n",
        "\n",
        "Ans = KNN and PCA complement each other effectively in a single pipeline, particularly when dealing with high-dimensional datasets. Here's how:\n",
        "\n",
        "1.  **Addressing the Curse of Dimensionality:** PCA is a powerful technique for dimensionality reduction. By applying PCA before KNN, you can reduce the number of features in the dataset. This directly mitigates the negative effects of the Curse of Dimensionality on KNN performance, such as increased sparsity, less reliable distance metrics, and higher computational cost.\n",
        "2.  **Improving KNN Performance:** Reducing dimensionality with PCA can lead to a more meaningful \"nearest neighbor\" search for KNN. In a lower-dimensional space where data is less sparse, the distances between points are more informative, allowing KNN to find true neighbors and make more accurate predictions.\n",
        "3.  **Reducing Computational Load:** KNN's prediction phase involves calculating distances to all training points. In high dimensions, this is computationally expensive. By reducing the number of dimensions with PCA, the distance calculations become much faster, significantly reducing the overall computational cost of the KNN algorithm.\n",
        "4.  **Noise Reduction:** PCA can also help in reducing noise in the data. By keeping only the principal components that capture the most variance, you are essentially filtering out the less important dimensions, which may contain noise. This can further improve the robustness of the KNN model.\n",
        "5.  **Handling Multicollinearity:** If the original features are highly correlated, PCA can help by creating uncorrelated principal components. This can be beneficial for some distance metrics used in KNN.\n",
        "\n",
        "In summary, PCA acts as a preprocessing step that prepares high-dimensional data for KNN. It addresses the challenges posed by the Curse of Dimensionality, leading to improved performance, reduced computational cost, and potentially better generalization for the KNN algorithm. The pipeline typically involves applying PCA to the data first and then training and using the KNN model on the transformed, lower-dimensional data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset:\n",
        "Use the Wine Dataset from sklearn.datasets.load_wine().\n",
        "Question 6: Train a KNN Classifier on the Wine dataset with and without feature\n",
        "scaling. Compare model accuracy in both cases.**"
      ],
      "metadata": {
        "id": "zaTV-wyWSAJ1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73ba1624"
      },
      "source": [
        "# Task\n",
        "Train a KNN Classifier on the Wine dataset from `sklearn.datasets.load_wine()` with and without feature scaling and compare the model accuracy in both cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "003f89e2"
      },
      "source": [
        "## Train knn without scaling\n",
        "\n",
        "### Subtask:\n",
        "Train a KNN classifier on the original data and evaluate its accuracy on the test set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6909aa5"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires loading the wine dataset, splitting it into training and testing sets, training a KNN classifier on the training data, making predictions on the test data, and calculating the accuracy. These steps can be grouped into a single code block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b52e3b23",
        "outputId": "e308e337-d124-4926-846f-480c0e1b9ea8"
      },
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize a KNeighborsClassifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Train the KNN classifier on the training data\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy_original = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy without scaling: {accuracy_original}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.7222222222222222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4229ed7"
      },
      "source": [
        "## Apply feature scaling\n",
        "\n",
        "### Subtask:\n",
        "Scale the features of the training and testing sets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c7127fb"
      },
      "source": [
        "**Reasoning**:\n",
        "Scale the training and testing data using StandardScaler as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f571765d",
        "outputId": "0762dd02-1464-4cc2-a578-695a7c954102"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Instantiate StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on the training data X_train\n",
        "scaler.fit(X_train)\n",
        "\n",
        "# Transform both X_train and X_test\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"Data scaled successfully.\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data scaled successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08c0b4b9"
      },
      "source": [
        "## Train knn with scaling\n",
        "\n",
        "### Subtask:\n",
        "Train a KNN classifier on the scaled data and evaluate its accuracy on the scaled test set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "640ee95d"
      },
      "source": [
        "**Reasoning**:\n",
        "Initialize a KNN classifier, train it on the scaled data, make predictions on the scaled test data, and calculate the accuracy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3748663a",
        "outputId": "f36485ed-22ce-4284-c62e-59bfc8f78cc0"
      },
      "source": [
        "# Initialize a new KNeighborsClassifier\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Train the KNN classifier on the scaled training data\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the scaled test data\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "\n",
        "# Calculate the accuracy of the model with scaling\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Print the accuracy of the model with scaling\n",
        "print(f\"Accuracy with scaling: {accuracy_scaled}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with scaling: 0.9444444444444444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ab73a0f"
      },
      "source": [
        "## Compare the accuracies of the two models.\n",
        "\n",
        "### Subtask:\n",
        "Compare the accuracies of the two models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66509de1"
      },
      "source": [
        "**Reasoning**:\n",
        "Print the calculated accuracies for both cases and provide a brief comparison.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fd15f73",
        "outputId": "7b9fc6b9-74ed-42fb-f706-38602ad5f17d"
      },
      "source": [
        "print(f\"Accuracy without scaling: {accuracy_original}\")\n",
        "print(f\"Accuracy with scaling: {accuracy_scaled}\")\n",
        "\n",
        "if accuracy_scaled > accuracy_original:\n",
        "    print(\"\\nFeature scaling significantly improved the accuracy of the KNN model for this dataset.\")\n",
        "elif accuracy_scaled < accuracy_original:\n",
        "    print(\"\\nFeature scaling slightly decreased the accuracy of the KNN model for this dataset.\")\n",
        "else:\n",
        "    print(\"\\nFeature scaling did not change the accuracy of the KNN model for this dataset.\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.7222222222222222\n",
            "Accuracy with scaling: 0.9444444444444444\n",
            "\n",
            "Feature scaling significantly improved the accuracy of the KNN model for this dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15d878a1"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The accuracy of the KNN model without feature scaling was found to be approximately 0.722.\n",
        "*   The accuracy of the KNN model with feature scaling applied was approximately 0.944.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Feature scaling significantly improved the accuracy of the KNN model for this dataset, increasing it from 72.2% to 94.4%. This highlights the importance of scaling for distance-based algorithms like KNN.\n",
        "*   Further exploration could involve testing different scaling methods or optimizing the `n_neighbors` parameter for both the scaled and unscaled data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7: Train a PCA model on the Wine dataset and print the explained variance\n",
        "ratio of each principal component.**"
      ],
      "metadata": {
        "id": "LKx7GNHlSxaP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2e80212",
        "outputId": "23919619-fbcd-4511-f4b2-aa36e8978641"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_wine\n",
        "\n",
        "# Load the wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA()\n",
        "pca.fit(X)\n",
        "\n",
        "# Print the explained variance ratio of each principal component\n",
        "print(\"Explained variance ratio of each principal component:\")\n",
        "for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
        "    print(f\"Principal Component {i+1}: {ratio:.4f}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained variance ratio of each principal component:\n",
            "Principal Component 1: 0.9981\n",
            "Principal Component 2: 0.0017\n",
            "Principal Component 3: 0.0001\n",
            "Principal Component 4: 0.0001\n",
            "Principal Component 5: 0.0000\n",
            "Principal Component 6: 0.0000\n",
            "Principal Component 7: 0.0000\n",
            "Principal Component 8: 0.0000\n",
            "Principal Component 9: 0.0000\n",
            "Principal Component 10: 0.0000\n",
            "Principal Component 11: 0.0000\n",
            "Principal Component 12: 0.0000\n",
            "Principal Component 13: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2\n",
        "components). Compare the accuracy with the original dataset.**"
      ],
      "metadata": {
        "id": "HWPmqXedToWz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcc3d98f"
      },
      "source": [
        "### Step 5: Compare accuracies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc29aecd",
        "outputId": "d9460e76-c639-49e6-9308-46e40d7b680a"
      },
      "source": [
        "print(f\"Accuracy with original data: {accuracy_original}\")\n",
        "print(f\"Accuracy with PCA (2 components): {accuracy_pca}\")\n",
        "\n",
        "if accuracy_pca > accuracy_original:\n",
        "    print(\"\\nUsing PCA with 2 components improved the accuracy of the KNN model.\")\n",
        "elif accuracy_pca < accuracy_original:\n",
        "    print(\"\\nUsing PCA with 2 components slightly decreased the accuracy of the KNN model.\")\n",
        "else:\n",
        "    print(\"\\nUsing PCA with 2 components did not change the accuracy of the KNN model.\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with original data: 0.7222222222222222\n",
            "Accuracy with PCA (2 components): 0.7222222222222222\n",
            "\n",
            "Using PCA with 2 components did not change the accuracy of the KNN model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3bbff71"
      },
      "source": [
        "### Step 4: Evaluate KNN on PCA-transformed data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae446670",
        "outputId": "22c3c0b3-b559-4d68-8b8d-7a2ef676f2ac"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Make predictions on the PCA-transformed test data\n",
        "y_pred_pca = knn_pca.predict(X_pca_test)\n",
        "\n",
        "# Calculate the accuracy of the model on the PCA-transformed data\n",
        "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "print(f\"Accuracy with PCA (2 components): {accuracy_pca}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with PCA (2 components): 0.7222222222222222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7bdd69f"
      },
      "source": [
        "### Step 3: Train KNN on PCA-transformed data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a4265ab",
        "outputId": "f98d5b55-4227-4f6b-ca74-1b9174140c2c"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Initialize a KNeighborsClassifier\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Train the KNN classifier on the PCA-transformed training data\n",
        "knn_pca.fit(X_pca_train, y_train)\n",
        "\n",
        "print(\"KNN classifier trained on PCA-transformed data.\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN classifier trained on PCA-transformed data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe6189fb"
      },
      "source": [
        "### Step 2: Split PCA-transformed data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d556f6ad",
        "outputId": "b49d4441-e70e-422e-fbd6-6536d32ccc87"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the PCA-transformed dataset into training and testing sets\n",
        "X_pca_train, X_pca_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Shape of PCA training data:\", X_pca_train.shape)\n",
        "print(\"Shape of PCA testing data:\", X_pca_test.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of PCA training data: (142, 2)\n",
            "Shape of PCA testing data: (36, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8b03ca8"
      },
      "source": [
        "### Step 1: Apply PCA with 2 components"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef1ca133",
        "outputId": "b7447403-837f-4cdd-f02c-6e5ab6cdb44d"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Apply PCA, retaining the top 2 principal components\n",
        "pca_2 = PCA(n_components=2)\n",
        "X_pca = pca_2.fit_transform(X)\n",
        "\n",
        "print(\"Shape of original data:\", X.shape)\n",
        "print(\"Shape of PCA-transformed data:\", X_pca.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of original data: (178, 13)\n",
            "Shape of PCA-transformed data: (178, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9: Train a KNN Classifier with different distance metrics (euclidean,\n",
        "manhattan) on the scaled Wine dataset and compare the results.**"
      ],
      "metadata": {
        "id": "7vABx2e5VQ3w"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5476fc9"
      },
      "source": [
        "# Task\n",
        "Train KNN classifiers with 'euclidean' and 'manhattan' distance metrics on the scaled Wine dataset, compare their accuracies, and summarize the findings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb3b7f78"
      },
      "source": [
        "## Train knn classifier (euclidean)\n",
        "\n",
        "### Subtask:\n",
        "Train a KNN classifier using the 'euclidean' distance metric on the scaled data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7b46570"
      },
      "source": [
        "**Reasoning**:\n",
        "Initialize and train a KNN classifier with the euclidean distance metric on the scaled training data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ed8b247",
        "outputId": "ccca9e74-5839-43d1-c86e-1cf4990b9a56"
      },
      "source": [
        "# Initialize a KNeighborsClassifier with euclidean distance\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "\n",
        "# Train the KNN classifier on the scaled training data\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"KNN classifier with euclidean distance trained on scaled data.\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN classifier with euclidean distance trained on scaled data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9e81422"
      },
      "source": [
        "## Evaluate knn accuracy (euclidean)\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the accuracy of the KNN model with the 'euclidean' distance metric.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01d48963"
      },
      "source": [
        "**Reasoning**:\n",
        "Evaluate the accuracy of the KNN model with the 'euclidean' distance metric on the scaled test set and print the result.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "154b2b39",
        "outputId": "4f372932-3faf-48f8-ec6f-6c30f3c21393"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Make predictions on the scaled test data using the euclidean model\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test_scaled)\n",
        "\n",
        "# Calculate the accuracy of the model with euclidean distance\n",
        "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"Accuracy with euclidean distance: {accuracy_euclidean}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with euclidean distance: 0.9444444444444444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f843a28f"
      },
      "source": [
        "## Train knn classifier (manhattan)\n",
        "\n",
        "### Subtask:\n",
        "Train a KNN classifier using the 'manhattan' distance metric on the scaled data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6b07627"
      },
      "source": [
        "**Reasoning**:\n",
        "Initialize and train a KNN classifier with the 'manhattan' distance metric on the scaled training data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d97ad906",
        "outputId": "a7e83468-0660-42ef-8c7a-14f463d5e92d"
      },
      "source": [
        "# Initialize a KNeighborsClassifier with manhattan distance\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "\n",
        "# Train the KNN classifier on the scaled training data\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"KNN classifier with manhattan distance trained on scaled data.\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN classifier with manhattan distance trained on scaled data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9104b77f"
      },
      "source": [
        "## Evaluate knn accuracy (manhattan)\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the accuracy of the KNN model with the 'manhattan' distance metric.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "303d892f"
      },
      "source": [
        "**Reasoning**:\n",
        "Evaluate the accuracy of the KNN model with the 'manhattan' distance metric on the scaled test data and print the result.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57aa6da0",
        "outputId": "58ff8cfb-d1dd-4f23-ed62-68fa9c475e33"
      },
      "source": [
        "# Make predictions on the scaled test data using the manhattan model\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
        "\n",
        "# Calculate the accuracy of the model with manhattan distance\n",
        "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"Accuracy with manhattan distance: {accuracy_manhattan}\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with manhattan distance: 0.9444444444444444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "489475c0"
      },
      "source": [
        "## Compare model accuracies\n",
        "\n",
        "### Subtask:\n",
        "Compare the accuracies of the two models trained with different distance metrics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16e38ddf"
      },
      "source": [
        "**Reasoning**:\n",
        "Print the accuracies obtained with euclidean and manhattan distance metrics and compare them.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c8c749e",
        "outputId": "74185d60-a50a-45f0-8287-ba2458740056"
      },
      "source": [
        "print(f\"Accuracy with euclidean distance: {accuracy_euclidean}\")\n",
        "print(f\"Accuracy with manhattan distance: {accuracy_manhattan}\")\n",
        "\n",
        "if accuracy_euclidean > accuracy_manhattan:\n",
        "    print(\"\\nEuclidean distance resulted in higher accuracy.\")\n",
        "elif accuracy_manhattan > accuracy_euclidean:\n",
        "    print(\"\\nManhattan distance resulted in higher accuracy.\")\n",
        "else:\n",
        "    print(\"\\nBoth distance metrics resulted in the same accuracy.\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with euclidean distance: 0.9444444444444444\n",
            "Accuracy with manhattan distance: 0.9444444444444444\n",
            "\n",
            "Both distance metrics resulted in the same accuracy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8c9b4a2"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The KNN classifier trained with the 'euclidean' distance metric on the scaled data achieved an accuracy of approximately 0.9444.\n",
        "*   The KNN classifier trained with the 'manhattan' distance metric on the scaled data also achieved an accuracy of approximately 0.9444.\n",
        "*   Both distance metrics resulted in the same classification accuracy for the KNN model on this specific dataset.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   For this dataset, the choice between Euclidean and Manhattan distance metrics for KNN does not significantly impact accuracy. Further investigation could explore if this holds true with different `n_neighbors` values or other datasets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: You are working with a high-dimensional gene expression dataset to\n",
        "classify patients with different types of cancer.\n",
        "Due to the large number of features and a small number of samples, traditional models\n",
        "overfit.\n",
        "Explain how you would:\n",
        "● Use PCA to reduce dimensionality\n",
        "● Decide how many components to keep\n",
        "● Use KNN for classification post-dimensionality reduction\n",
        "● Evaluate the model\n",
        "● Justify this pipeline to your stakeholders as a robust solution for real-world\n",
        "biomedical data\n",
        "(Include your Python code and output in the code box below.)**"
      ],
      "metadata": {
        "id": "HfLcNbZ_V9gj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "245d8a25"
      },
      "source": [
        "## Addressing High-Dimensional Gene Expression Data with PCA and KNN\n",
        "\n",
        "In the scenario of a high-dimensional gene expression dataset with a limited number of samples, where traditional models are prone to overfitting, a pipeline combining PCA for dimensionality reduction and KNN for classification is a robust approach. Here's how you would implement and justify this:\n",
        "\n",
        "**1. Use PCA to reduce dimensionality:**\n",
        "\n",
        "High-dimensional data often contains redundant or noisy features. PCA helps by transforming the data into a lower-dimensional space while preserving the most important variance. This is achieved by identifying the principal components, which are orthogonal directions in the feature space that capture the maximum variance.\n",
        "\n",
        "**2. Decide how many components to keep:**\n",
        "\n",
        "Selecting the optimal number of principal components is crucial. Several methods can be used:\n",
        "\n",
        "*   **Explained Variance Ratio:** Plot the cumulative explained variance ratio as a function of the number of components. Choose the number of components that explain a significant portion of the total variance (e.g., 95%).\n",
        "*   **Scree Plot:** Plot the eigenvalues (variance explained by each component) in descending order. Look for an \"elbow\" in the plot, where the rate of decrease in eigenvalues slows down. Components before the elbow are typically retained.\n",
        "*   **Cross-validation:** Train and evaluate the KNN model with different numbers of principal components using cross-validation. Select the number of components that yields the best performance.\n",
        "\n",
        "**3. Use KNN for classification post-dimensionality reduction:**\n",
        "\n",
        "After reducing the dimensionality using PCA, you would train a KNN classifier on the transformed data. KNN is suitable here because:\n",
        "\n",
        "*   It's a non-parametric algorithm, meaning it doesn't make assumptions about the underlying data distribution, which can be beneficial for complex biological data.\n",
        "*   With reduced dimensionality, the \"curse of dimensionality\" is mitigated, and KNN can find meaningful nearest neighbors more effectively.\n",
        "\n",
        "**4. Evaluate the model:**\n",
        "\n",
        "Standard classification evaluation metrics should be used:\n",
        "\n",
        "*   **Accuracy:** The proportion of correctly classified samples.\n",
        "*   **Precision, Recall, F1-score:** These metrics are particularly important in biomedical applications where misclassifications can have significant consequences.\n",
        "*   **Cross-validation:** Use k-fold cross-validation to get a more reliable estimate of the model's performance on unseen data and to avoid overfitting to the training set.\n",
        "\n",
        "**5. Justify this pipeline to your stakeholders:**\n",
        "\n",
        "This PCA+KNN pipeline is a robust solution for real-world biomedical data due to:\n",
        "\n",
        "*   **Addressing Overfitting:** PCA effectively reduces the number of features, which is critical for preventing overfitting in scenarios with limited samples and high dimensionality.\n",
        "*   **Improved Computational Efficiency:** Reducing dimensionality makes the KNN algorithm computationally less expensive, which is important for large datasets.\n",
        "*   **Noise Reduction:** PCA can help filter out noise in the data by focusing on the components that capture the most variance.\n",
        "*   **Interpretability (to some extent):** While the principal components themselves may not have direct biological interpretations, analyzing the loadings of the original features on the important components can provide insights into which genes contribute most to the variation in the data.\n",
        "*   **Established Techniques:** Both PCA and KNN are well-established and widely used techniques in machine learning and data analysis, which can increase confidence in the results.\n",
        "\n",
        "Here's a conceptual Python code example demonstrating this pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23f39598",
        "outputId": "a3c1860e-97f0-4b20-e13e-ba22d0fa32a8"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Conceptual Data (replace with your actual gene expression data)\n",
        "# X: High-dimensional gene expression data (samples x features)\n",
        "# y: Cancer type labels (samples)\n",
        "# Assume X and y are loaded from your dataset\n",
        "\n",
        "# For demonstration, let's create some dummy data\n",
        "n_samples = 100\n",
        "n_features = 1000\n",
        "X = np.random.rand(n_samples, n_features)\n",
        "y = np.random.randint(0, 3, n_samples) # 3 different cancer types\n",
        "\n",
        "# 1. Scale the data (important for PCA)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 2. Use PCA to reduce dimensionality\n",
        "# Decide number of components (e.g., based on explained variance)\n",
        "pca = PCA(n_components=0.95) # Retain 95% of variance\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "print(f\"Original dimensions: {X.shape}\")\n",
        "print(f\"Reduced dimensions after PCA: {X_pca.shape}\")\n",
        "print(f\"Explained variance by selected components: {np.sum(pca.explained_variance_ratio_):.4f}\")\n",
        "\n",
        "# 3. Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# 4. Use KNN for classification post-dimensionality reduction\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Train the model\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# 5. Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nKNN Accuracy after PCA: {accuracy:.4f}\")\n",
        "\n",
        "# More detailed evaluation\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Cross-validation for robust evaluation\n",
        "cv_scores = cross_val_score(knn, X_pca, y, cv=5) # 5-fold cross-validation\n",
        "print(f\"\\nCross-validation Accuracy (5-fold): {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores):.4f})\")\n",
        "\n",
        "# 6. Justification to stakeholders (covered in the markdown explanation)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dimensions: (100, 1000)\n",
            "Reduced dimensions after PCA: (100, 90)\n",
            "Explained variance by selected components: 0.9512\n",
            "\n",
            "KNN Accuracy after PCA: 0.3200\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.20      0.14      0.17         7\n",
            "           1       0.38      0.62      0.48         8\n",
            "           2       0.29      0.20      0.24        10\n",
            "\n",
            "    accuracy                           0.32        25\n",
            "   macro avg       0.29      0.32      0.29        25\n",
            "weighted avg       0.29      0.32      0.29        25\n",
            "\n",
            "\n",
            "Cross-validation Accuracy (5-fold): 0.3200 (+/- 0.0748)\n"
          ]
        }
      ]
    }
  ]
}